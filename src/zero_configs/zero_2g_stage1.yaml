# ZeRO-1 (shard optimizer states); good baseline vs DDP on 2 GPUs
train_micro_batch_size_per_gpu: 16     # match CONF["10m"]["batch_size"]
gradient_accumulation_steps: 1
bf16:
  enabled: true                        # set to false if GPUs lack bf16
fp16:
  enabled: false

zero_optimization:
  stage: 1
  overlap_comm: true
  contiguous_gradients: true
  reduce_scatter: true                 # safe on stage >=2; harmless on stage 1
  reduce_bucket_size: 50000000         # 50M elements (~200MB fp32, ~100MB bf16)
  allgather_bucket_size: 50000000

gradient_clipping: 1.0
wall_clock_breakdown: true             # adds DS timing to logs

# optional knobs to uncomment to stress memory further:
# activation_checkpointing:
#   partition_activations: true
#   contiguous_memory_optimization: true
#   cpu_checkpointing: false

